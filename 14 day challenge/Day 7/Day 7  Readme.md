# ğŸš€ Day 7 â€“ Databricks 14 Days AI Challenge  
ğŸ“… 16/01/26  
âš™ï¸ Jobs & Declarative Pipelines

---

## ğŸ“– Learn
- Databricks Jobs vs notebooks  
- Multi-task workflows  
- Parameters & scheduling  
- Error handling  

---

## ğŸ› ï¸ Tasks
1. Add parameter widgets to notebooks  
2. Create multi-task job (Bronze â†’ Silver â†’ Gold)  
3. Set up dependencies  
4. Schedule execution  

---

## ğŸ§‘â€ğŸ’» Hands-On Work
- Converted batch and stream notebooks into **Jobs** with parameter widgets.  
- Built a **multi-task workflow** chaining Bronze â†’ Silver â†’ Gold layers.  
- Defined **dependencies** to ensure correct execution order.  
- Scheduled jobs for automated refresh.  
- Migrated both batch and stream pipelines into **Declarative Delta Pipelines** for simplified orchestration.  

---

## ğŸ“Š Insights
- **Jobs** provide orchestration and scheduling.  
- **Declarative Pipelines** simplify incremental processing and monitoring.  
- Combining both ensures **production-ready pipelines** for batch and stream workloads.  

---

## ğŸ™Œ Reflection
Day 7 highlighted how to move from **exploratory notebooks** to **automated, declarative pipelines**.  
This shift makes Medallion Architecture **scalable, resilient, and production-ready**.

---

#DatabricksWithIDC #AIChallenge #PySpark #BigData #DatabricksJobs #DeltaPipelines #Day7