# ğŸš€ Day 3 â€“ Databricks 14 Days AI Challenge  
ğŸ“… 11/01/26  
âš™ï¸ PySpark Transformations Deep Dive

---

## ğŸ“– Learn
- PySpark vs Pandas â†’ scaling from singleâ€‘node analysis to distributed big data processing  
- Joins â†’ combining datasets with inner, left, right, and outer joins  
- Window Functions â†’ running totals, rankings, and cumulative analytics without collapsing data  
- UDFs â†’ extending PySpark with custom Python logic for derived features  

---

## ğŸ› ï¸ Tasks
1. Load full e-commerce dataset  
2. Perform complex joins  
3. Calculate running totals with window functions  
4. Create derived features  

---

## ğŸ§‘â€ğŸ’» Hands-On Work
- Loaded October + November e-commerce datasets into Databricks.  
- Created DataFrames for **purchase**, **cart**, and **view** events.  
- Aggregated brand-level totals and applied **window functions** (`dense_rank`) for ranking.  
- Joined purchase, cart, and view DataFrames to build a consolidated view.  
- Engineered **conversion rate features**:  
  - Cart â†’ Purchase Conversion Rate  
  - View â†’ Purchase Conversion Rate  
- Implemented a **UDF** to classify brands into performance categories.  

---

## ğŸ“Š Insights
- Top 10 brands by purchase revenue with conversion rates.  
- Top 10 brands by cart-to-purchase conversion efficiency.  
- Top 10 brands by view-to-purchase conversion efficiency.  
- UDF-based **Performance Flag** adds recruiter-friendly business context.  

---

## ğŸ™Œ Reflection
Day 3 highlighted how PySpark transformations, window functions, and UDFs together enable scalable, customizable analytics pipelines that connect raw event logs to actionable business metrics.

---

#DatabricksWithIDC #AIChallenge #PySpark #BigData #DataEngineering #Day3