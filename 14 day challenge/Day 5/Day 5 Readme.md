# ğŸš€ Day 5 â€“ Databricks 14 Days AI Challenge  
ğŸ“… 14/01/26  
âš™ï¸ Delta Lake Advanced: Version Control & Optimization

---

## ğŸ“– Learn
- Time travel (version history)  
- MERGE operations (upserts)  
- OPTIMIZE & ZORDER  
- VACUUM for cleanup  

---

## ğŸ› ï¸ Tasks
1. Implement incremental MERGE  
2. Query historical versions  
3. Optimize tables  
4. Clean old files  

---

## ğŸ§‘â€ğŸ’» Hands-On Work
- Created new Delta datasets and applied **MERGE** for incremental upserts.  
- Queried historical versions using **time travel** (`versionAsOf`, `timestampAsOf`).  
- Applied **OPTIMIZE** to reduce 156 fragmented files down to 26 compact files.  
- Used **ZORDER** on `brand` and `user_id` to reduce query scans from 129 files â†’ 26 â†’ 4.  
- Ran **VACUUM** with custom retention to clean obsolete files.  

---

## ğŸ“Š Insights
- **Time travel** enables rollback and auditing.  
- **MERGE** ensures idempotent ingestion.  
- **OPTIMIZE & ZORDER** drastically improve query performance and reduce file scans.  
- **VACUUM** keeps storage lean and efficient.  

---

## ğŸ™Œ Reflection
Day 5 highlighted how Delta Lake goes beyond governance â€” it enables **performance tuning and lifecycle management**.  
With time travel, MERGE, OPTIMIZE, ZORDER, and VACUUM, pipelines become **auditable, efficient, and productionâ€‘ready**.

---

#DatabricksWithIDC #AIChallenge #PySpark #BigData #DeltaLake #Day5